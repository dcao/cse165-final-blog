---
title: "Final Project Blog 4: Final Report"
date: 2023-06-13
---

This is the final report for our CSE 165 final project: Ordering Simulator 2023.

Our video is available here: [https://www.youtube.com/watch?v=exwAzIMklx4](https://www.youtube.com/watch?v=exwAzIMklx4).

## Authors

This project was done by **David Cao, Jeremy Lei, and Thomas Wang**.

## Updates from our last blog

The first main implementation update was that we improved voice support by using Wit.ai's Composer library to use natural language processing to produce a more in-depth conversation with the user. Adding this feature allows for a natural conversation flow that adapts to user needs and inputs. We use the Oculus Voice SDK to automatically perform speech-to-text.

Additionally, we've fleshed out the game aspect of this experience and implemented all of the other requisite parts of the application, including a full game flow for trying to order items, as well as other auxiliary systems for making the game work (system control with the floating UI, selection & manipulation of the in-game menu, locomotion using our movement system from Project 1, etc.).

In other words, we finished everything we needed to to make this a fully-fledged virtual experience!

## Acknowledgments

For this project, we limited ourselves to using the capabilities provided by the Oculus Integration SDK (v51) and the Wit.ai Composer API integration (v51) with the Oculus Voice SDK.

In particular, for each skill item:

- Locomotion/movement is done with our own code, from project 1. The same applies for our selection and manipulation routines.
- Gesture recognition (for the hand tracking point) is done with our gesture recognition code from project 3.
- The UI uses Unity's standard Canvas/UI affordances.
- Voice recognition is done with the Oculus Voice SDK and the Wit.ai Composer API integration package.

## References used

- Hand/UI interaction: `RayExamples` scene (from Oculus Integration SDK)
  - https://developer.oculus.com/documentation/unity/unity-isdk-canvas-integration/#PointableCanvasModule
  - https://www.youtube.com/watch?v=waFJcar3cwM (legacy/old)
  - https://forum.unity.com/threads/how-to-oculus-quest-hand-tracking-pointerpose-pinch.1170538/
  - https://www.youtube.com/watch?v=BYS2NN1bWS4
  - https://developer.oculus.com/documentation/unity/unity-isdk-ray-interaction/
- Grabbing things: https://developer.oculus.com/documentation/unity/unity-isdk-touch-hand-grab-interaction
- voice stuff
  - https://developer.oculus.com/documentation/unity/voice-sdk-transcription/
  - https://developer.oculus.com/documentation/unity/voice-sdk-visual-feedback/
- gesture stuff
  - https://communityforums.atmeta.com/t5/Unity-VR-Development/Getting-head-and-finger-orientation/td-p/959591
  - https://www.youtube.com/watch?v=lBzwUKQ3tbw
